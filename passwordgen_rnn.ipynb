{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oKeos69LGynz",
    "outputId": "62dbe615-26d4-4bf3-c624-c54fb3c15ffe",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pytorch-lightning in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (1.6.2)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from pytorch-lightning) (4.63.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from pytorch-lightning) (21.3)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from pytorch-lightning) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from pytorch-lightning) (4.1.1)\n",
      "Requirement already satisfied: pyDeprecate<0.4.0,>=0.3.1 in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from pytorch-lightning) (0.3.2)\n",
      "Requirement already satisfied: torchmetrics>=0.4.1 in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from pytorch-lightning) (0.8.1)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from pytorch-lightning) (2022.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from pytorch-lightning) (1.21.5)\n",
      "Requirement already satisfied: torch>=1.8.* in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from pytorch-lightning) (1.10.2)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from pytorch-lightning) (2.8.0)\n",
      "Requirement already satisfied: requests in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.27.1)\n",
      "Requirement already satisfied: aiohttp in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.8.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from packaging>=17.0->pytorch-lightning) (3.0.7)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.6)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/grads/b/bhanu/.local/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.19.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (2.0.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.37.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.6.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/grads/b/bhanu/.local/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.44.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/grads/b/bhanu/.local/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (60.9.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (2.6.0)\n",
      "Requirement already satisfied: six in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (5.0.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (4.11.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.0.12)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.7.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.4.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (6.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/grads/b/bhanu/miniconda3/envs/myclone/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extension horovod.torch has not been built: /home/grads/b/bhanu/.local/lib/python3.8/site-packages/horovod/torch/mpi_lib/_mpi_lib.cpython-38-x86_64-linux-gnu.so not found\n",
      "If this is not expected, reinstall Horovod with HOROVOD_WITH_PYTORCH=1 to debug the build error.\n",
      "Warning! MPI libs are missing, but python applications are still avaiable.\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-lightning\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import lr_scheduler, Adam\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data downloading and pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Ibzhgi8_HUP5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-05-03 19:24:52--  https://transfer.sh/6KUxG4/rockyou.txt\n",
      "Resolving transfer.sh (transfer.sh)... 144.76.136.153, 2a01:4f8:200:1097::2\n",
      "Connecting to transfer.sh (transfer.sh)|144.76.136.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 139921497 (133M) [text/plain]\n",
      "Saving to: ‘rockyou.txt’\n",
      "\n",
      "rockyou.txt         100%[===================>] 133.44M  22.0MB/s    in 7.0s    \n",
      "\n",
      "2022-05-03 19:25:00 (19.1 MB/s) - ‘rockyou.txt’ saved [139921497/139921497]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Uncomment if you are running on new server to download\n",
    "# !wget https://transfer.sh/6KUxG4/rockyou.txt\n",
    "import re\n",
    "data = re.findall('\\w+', open('rockyou.txt', encoding='latin-1').read())\n",
    "\n",
    "all_letters = [\"<pad>\"] + list(string.printable) + [\"<eos>\"]\n",
    "n_letters = len(all_letters)\n",
    "n_letters\n",
    "\n",
    "clean_data = []\n",
    "for strs in data:\n",
    "  if strs.isascii():\n",
    "    clean_data.append(strs)\n",
    "\n",
    "data = clean_data   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "auwHztBgIonC"
   },
   "outputs": [],
   "source": [
    "# String to int and int to str dict and list\n",
    "stoi = {letter : idx for idx, letter in enumerate(all_letters)}\n",
    "itos = [letter for idx, letter in enumerate(all_letters)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-pOWQz7pJk-C"
   },
   "outputs": [],
   "source": [
    "class NamesDataset(Dataset):\n",
    "    def __init__(self, data, stoi, eos_token=\"<eos>\"):\n",
    "        self.stoi = stoi\n",
    "        self.eos_token = eos_token\n",
    "        self.n_tokens = len(self.stoi)\n",
    "        \n",
    "        self.names = data\n",
    "\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        name = self.names[item]\n",
    "        \n",
    "        input_tensor = torch.tensor([stoi[char] for char in name])\n",
    "        target_tensor = torch.tensor([stoi[char] for char in list(name[1:])+[self.eos_token]])\n",
    "        \n",
    "        item_dict = {\n",
    "        \"name\": name,\n",
    "        \"input_tensor\": input_tensor,\n",
    "        \"target_tensor\": target_tensor}\n",
    "        \n",
    "        \n",
    "        return item_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qo1SieHdKMkT",
    "outputId": "1fd85d16-7951-4c94-f27e-351599c97800"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': '123456',\n",
       " 'input_tensor': tensor([2, 3, 4, 5, 6, 7]),\n",
       " 'target_tensor': tensor([  3,   4,   5,   6,   7, 101])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = NamesDataset(data, stoi)\n",
    "\n",
    "ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collate to pad on batch with various length inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "UpPoi55YKPmr"
   },
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    def merge(sequences):\n",
    "        \"https://github.com/yunjey/seq2seq-dataloader/blob/master/data_loader.py\"\n",
    "        \n",
    "        lengths = [len(seq) for seq in sequences]\n",
    "        padded_seqs = torch.zeros(len(sequences), max(lengths)).long()\n",
    "        for i, seq in enumerate(sequences):\n",
    "            end = lengths[i]\n",
    "            padded_seqs[i, :end] = seq[:end]\n",
    "        return padded_seqs, lengths\n",
    "       \n",
    "    names = [x[\"name\"] for x in data]          \n",
    "    \n",
    "    input_tensors = [x[\"input_tensor\"] for x in data]\n",
    "    input_tensors, _ = merge(input_tensors)\n",
    "    \n",
    "    target_tensors = [x[\"target_tensor\"] for x in data]\n",
    "    target_tensors, _ = merge(target_tensors)\n",
    "    \n",
    "    return names, input_tensors, target_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8OOYW1gNMYgs"
   },
   "outputs": [],
   "source": [
    "dl = DataLoader(ds, batch_size=1, collate_fn=collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KsclrqDrMblt",
    "outputId": "2b99d99c-919f-4d78-9ed3-b7bf4d3b07e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['0892273609'],\n",
       " tensor([[ 1,  9, 10,  3,  3,  8,  4,  7,  1, 10]]),\n",
       " tensor([[  9,  10,   3,   3,   8,   4,   7,   1,  10, 101]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "xBDMoEtPMyiN"
   },
   "outputs": [],
   "source": [
    "class NamesDatamodule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.data = re.findall('\\w+', open('rockyou.txt', encoding='latin-1').read())\n",
    "        self.all_letters = [\"<pad>\"] + list(string.printable) + [\"<eos>\"]\n",
    "        self.stoi = {letter : idx for idx, letter in enumerate(self.all_letters)}\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_set = NamesDataset(data, self.stoi)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True, collate_fn=self.collate_fn)\n",
    "    \n",
    "    def collate_fn(self, data):\n",
    "        def merge(sequences):\n",
    "            \"https://github.com/yunjey/seq2seq-dataloader/blob/master/data_loader.py\"\n",
    "\n",
    "            lengths = [len(seq) for seq in sequences]\n",
    "            padded_seqs = torch.zeros(len(sequences), max(lengths)).long()\n",
    "            for i, seq in enumerate(sequences):\n",
    "                end = lengths[i]\n",
    "                padded_seqs[i, :end] = seq[:end]\n",
    "            return padded_seqs, lengths\n",
    "\n",
    "        names = [x[\"name\"] for x in data]          \n",
    "\n",
    "        input_tensors = [x[\"input_tensor\"] for x in data]\n",
    "        input_tensors, _ = merge(input_tensors)\n",
    "\n",
    "        target_tensors = [x[\"target_tensor\"] for x in data]\n",
    "        target_tensors, _ = merge(target_tensors)\n",
    "        \n",
    "        item_dict = {\n",
    "                     \"names\": names, \n",
    "                     \"input_tensors\": input_tensors,\n",
    "                     \"target_tensors\": target_tensors}\n",
    "\n",
    "        return item_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN model module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "kH_XH6IgNvM4"
   },
   "outputs": [],
   "source": [
    "class RNN(pl.LightningModule):\n",
    "    lr = 5e-4\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, embeding_size, n_layers, output_size, p):\n",
    "        super().__init__()\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        \n",
    "        self.embeding = nn.Embedding(input_size, embeding_size)\n",
    "        self.lstm = nn.LSTM(embeding_size, hidden_size, n_layers, dropout=p)\n",
    "        self.out_fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p)\n",
    "        \n",
    "\n",
    "    def forward(self, batch_of_letter, hidden, cell):\n",
    "        ## letter level operations\n",
    "        \n",
    "        embeding = self.dropout(self.embeding(batch_of_letter))\n",
    "        # category_plus_letter = torch.cat((batch_of_category, embeding), 1)\n",
    "\n",
    "        #sequence_length = 1\n",
    "        category_plus_letter = embeding.unsqueeze(1)\n",
    "        \n",
    "        out, (hidden, cell) = self.lstm(category_plus_letter, (hidden, cell))\n",
    "        out = self.out_fc(out)\n",
    "        out = out.squeeze(1)\n",
    "        \n",
    "        return out, (hidden, cell)\n",
    "        \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), self.lr)\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        item_dict = batch\n",
    "        loss = 0\n",
    "\n",
    "        #to(device) needed due to some problem with PL\n",
    "        hidden = torch.zeros(self.n_layers, 1, self.hidden_size).to(self.device)\n",
    "        cell = torch.zeros(self.n_layers, 1, self.hidden_size).to(self.device)\n",
    "\n",
    "        #we loop over letters, single batch at the time \n",
    "        for t in range(item_dict[\"input_tensors\"].size(1)):\n",
    "            batch_of_letter = item_dict[\"input_tensors\"][:, t]\n",
    "            \n",
    "            output, (hidden, cell) = self(batch_of_letter, hidden, cell)\n",
    "            \n",
    "            loss += self.criterion(output, item_dict[\"target_tensors\"][:, t])\n",
    "\n",
    "        loss = loss/(t+1)\n",
    "\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "        cell = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "        \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381,
     "referenced_widgets": [
      "1981d2921cf2403f9b407aeb15d7d8f3",
      "198fa4428d7d4353a566d7fb92495104",
      "f9320318c08446309194bbccb644bff3",
      "a915fdf70eda46fab3d6cbff7418d4b3",
      "04e46282fd5c438699b058c6d0f856a4",
      "70ec3c94341943e496faf345150e836c",
      "9174153b52934414acefe6e284040d20",
      "f79deb8e3328426e9e2f3d028351c6a7",
      "a8c963f944374cedb839158564f17e0e",
      "6abed48d5ac841118c8d4832d90099c8",
      "bef65e992ede4e2094fec5ce098f30d1"
     ]
    },
    "id": "H6i1ga2iOpgA",
    "outputId": "e397bc6d-c18b-46a1-af7b-dd25f35b55c1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | criterion | CrossEntropyLoss | 0     \n",
      "1 | embeding  | Embedding        | 13.1 K\n",
      "2 | lstm      | LSTM             | 921 K \n",
      "3 | out_fc    | Linear           | 26.2 K\n",
      "4 | dropout   | Dropout          | 0     \n",
      "-----------------------------------------------\n",
      "960 K     Trainable params\n",
      "0         Non-trainable params\n",
      "960 K     Total params\n",
      "3.843     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4c70d4252f5428fb14bfda56be1b1cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dm = NamesDatamodule(1024)\n",
    "\n",
    "rnn_model = RNN(input_size=ds.n_tokens,\n",
    "            hidden_size=256,\n",
    "            embeding_size = 128, \n",
    "            n_layers=2,    \n",
    "            output_size=ds.n_tokens,\n",
    "            p=0.3)\n",
    "\n",
    "\n",
    "# Comment from here on while inference \n",
    "\n",
    "trainer = Trainer(max_epochs=3, \n",
    "                  logger=None,\n",
    "                  gpus=1,\n",
    "                 auto_scale_batch_size=True)\n",
    "\n",
    "trainer.fit(rnn_model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "id": "Yx0_MfPGY7VY"
   },
   "outputs": [],
   "source": [
    "max_length = 5\n",
    "\n",
    "# Sample from a category and starting letter\n",
    "def sample(start_letter, model,max_length):\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "        arr = [ds.stoi[c] for c in 'abc']\n",
    "        input = torch.tensor(ds.stoi[start_letter]).unsqueeze(0)\n",
    "#         input = torch.tensor(arr)\n",
    "        hidden, cell = model.init_hidden(1)\n",
    "#         print(input.shape)\n",
    "        output_name = start_letter\n",
    "\n",
    "        for i in range(max_length):\n",
    "            output, (hidden, cell) = model(input, hidden, cell)\n",
    "            topv, topi = output.topk(1)\n",
    "            topi = topi[0][0]\n",
    "            if topi == ds.stoi[\"<eos>\"]:\n",
    "                break\n",
    "            else:\n",
    "                letter = itos[topi]\n",
    "                output_name += letter\n",
    "                \n",
    "            input = torch.tensor(ds.stoi[letter]).unsqueeze(0)\n",
    "#                 arr.append(ds.stoi[letter])\n",
    "#                 input = torch.tensor(arr)\n",
    "\n",
    "        return output_name\n",
    "\n",
    "\n",
    "def samples(start_letter, num_gens, model,max_length):\n",
    "    arr = []\n",
    "    for i in range(num_gens):\n",
    "        arr.append(sample(start_letter, model,max_length))\n",
    "#         print(sample(start_letter, model,max_length))\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLEU Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# compute a very naïve BLEU score -- for educational purposes only\n",
    "\n",
    "def BLEU_star(refs, candidates):\n",
    "    \n",
    "    # tokenize the references and the candidate\n",
    "    refs = [ref.split() for ref in refs]\n",
    "    candidate = candidates.split()\n",
    "\n",
    "    # compute word frequencies for the references and the candidate\n",
    "    refs_counts = [Counter(ref) for ref in refs]\n",
    "    candidate_counts = Counter(candidate)\n",
    "\n",
    "    covered = 0\n",
    "    total = 0\n",
    "    \n",
    "    # compute the coverage for each word\n",
    "    for word, count in candidate_counts.items():\n",
    "        covered += min(count, max([ref[word] for ref in refs_counts]))\n",
    "        total += count\n",
    "    \n",
    "    # note: we can also use len(candidate) instead of total :)\n",
    "    return covered / total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Randomly sample 10000 passwords from whole dataset\n",
    "- In each of 10000 password take first character of each password and generate the a new passowrd\n",
    "- calculate bleu score on 10000 samples passwords as reference and 10000 generated passswords as hypothesis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "tsams = random.sample(data, 10000)\n",
    "\n",
    "cands = []\n",
    "for samp in tqdm(tsams):\n",
    "    cands.extend(samples(samp[0], 1 , rnn_model,max_length=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:02, 4768.25it/s]\n"
     ]
    }
   ],
   "source": [
    "final_scr = []\n",
    "for tup in tqdm(zip(tsams,cands)):\n",
    "    refstr = tup[0].replace(\"\", \" \")[1: -1]\n",
    "    hypostr = tup[1].replace(\"\", \" \")[1: -1]\n",
    "    final_scr.append(BLEU_star([refstr],hypostr))\n",
    "    \n",
    "print(sum(final_scr)/len(final_scr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36791666666666367"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(final_scr)/10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "passwordgen_rnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "04e46282fd5c438699b058c6d0f856a4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "1981d2921cf2403f9b407aeb15d7d8f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_198fa4428d7d4353a566d7fb92495104",
       "IPY_MODEL_f9320318c08446309194bbccb644bff3",
       "IPY_MODEL_a915fdf70eda46fab3d6cbff7418d4b3"
      ],
      "layout": "IPY_MODEL_04e46282fd5c438699b058c6d0f856a4"
     }
    },
    "198fa4428d7d4353a566d7fb92495104": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_70ec3c94341943e496faf345150e836c",
      "placeholder": "​",
      "style": "IPY_MODEL_9174153b52934414acefe6e284040d20",
      "value": "Epoch 0:   0%"
     }
    },
    "6abed48d5ac841118c8d4832d90099c8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70ec3c94341943e496faf345150e836c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9174153b52934414acefe6e284040d20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a8c963f944374cedb839158564f17e0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a915fdf70eda46fab3d6cbff7418d4b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6abed48d5ac841118c8d4832d90099c8",
      "placeholder": "​",
      "style": "IPY_MODEL_bef65e992ede4e2094fec5ce098f30d1",
      "value": " 17280/15013722 [05:00&lt;72:24:38, 57.53it/s, loss=2.74]"
     }
    },
    "bef65e992ede4e2094fec5ce098f30d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f79deb8e3328426e9e2f3d028351c6a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f9320318c08446309194bbccb644bff3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f79deb8e3328426e9e2f3d028351c6a7",
      "max": 15013722,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a8c963f944374cedb839158564f17e0e",
      "value": 17280
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
